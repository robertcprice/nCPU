#!/usr/bin/env python3
"""
Memory Oracle: LSTM-based Memory Access Predictor and Prefetcher

Phase 1 of the "Intelligent Dispatcher" architecture recommended by the
5-AI hybrid review. This module:

1. Tracks memory access patterns in a GPU-resident ring buffer
2. Uses LSTM to predict upcoming memory addresses
3. Prefetches predicted addresses to hide memory latency
4. Learns patterns online during execution

The key insight: 85% of slowdown in data-dependent loops is memory latency.
By predicting and prefetching memory accesses, we can overlap computation
with memory operations for massive speedups.

Patterns we can recognize:
- Sequential access (arrays, buffers)
- Strided access (struct fields, matrices)
- Pointer chasing (linked lists, trees) - hardest but most valuable
- Stack operations (function calls)
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple, List
from dataclasses import dataclass


@dataclass
class PrefetchStats:
    """Statistics for memory prediction performance."""
    total_accesses: int = 0
    predictions_made: int = 0
    hits: int = 0
    misses: int = 0
    prefetches_issued: int = 0
    bytes_prefetched: int = 0

    @property
    def hit_rate(self) -> float:
        return self.hits / max(1, self.predictions_made)

    @property
    def coverage(self) -> float:
        return self.predictions_made / max(1, self.total_accesses)


@dataclass
class DispatcherTelemetry:
    """
    Comprehensive telemetry for the Intelligent Dispatcher system.

    Aggregates metrics from:
    - Memory Oracle (LSTM predictions, prefetching)
    - Semantic Dispatcher (pattern detection, GPU kernels)
    - Safety systems (bounds checking, rejections)

    Phase 3 addition for observability and optimization.
    """
    # Oracle metrics
    oracle_total_accesses: int = 0
    oracle_predictions: int = 0
    oracle_hits: int = 0
    oracle_hit_rate: float = 0.0
    oracle_lstm_predictions: int = 0
    oracle_stride_detections: int = 0

    # Dispatcher metrics
    dispatcher_patterns_detected: int = 0
    dispatcher_instructions_saved: int = 0
    dispatcher_bytes_accelerated: int = 0
    dispatcher_try_calls: int = 0
    dispatcher_detection_rate: float = 0.0

    # Safety metrics
    safety_bounds_violations: int = 0
    safety_null_page_rejections: int = 0
    safety_overflow_rejections: int = 0
    safety_prefetch_rejections: int = 0

    # Adaptive threshold metrics
    adaptive_current_threshold: float = 0.7
    adaptive_adaptations: int = 0
    adaptive_window_hit_rate: float = 0.0

    # Trained model metrics
    trained_model_loaded: bool = False
    trained_model_pattern_accuracy: float = 0.0
    trained_current_pattern: str = 'unknown'
    trained_pattern_confidence: float = 0.0

    def to_dict(self) -> dict:
        """Convert telemetry to dictionary."""
        return {
            'oracle': {
                'total_accesses': self.oracle_total_accesses,
                'predictions': self.oracle_predictions,
                'hits': self.oracle_hits,
                'hit_rate': self.oracle_hit_rate,
                'lstm_predictions': self.oracle_lstm_predictions,
                'stride_detections': self.oracle_stride_detections,
            },
            'dispatcher': {
                'patterns_detected': self.dispatcher_patterns_detected,
                'instructions_saved': self.dispatcher_instructions_saved,
                'bytes_accelerated': self.dispatcher_bytes_accelerated,
                'try_calls': self.dispatcher_try_calls,
                'detection_rate': self.dispatcher_detection_rate,
            },
            'safety': {
                'bounds_violations': self.safety_bounds_violations,
                'null_page_rejections': self.safety_null_page_rejections,
                'overflow_rejections': self.safety_overflow_rejections,
                'prefetch_rejections': self.safety_prefetch_rejections,
            },
            'adaptive': {
                'current_threshold': self.adaptive_current_threshold,
                'adaptations': self.adaptive_adaptations,
                'window_hit_rate': self.adaptive_window_hit_rate,
            },
            'trained_model': {
                'loaded': self.trained_model_loaded,
                'pattern_accuracy': self.trained_model_pattern_accuracy,
                'current_pattern': self.trained_current_pattern,
                'pattern_confidence': self.trained_pattern_confidence,
            },
        }

    def print_summary(self, title: str = "INTELLIGENT DISPATCHER TELEMETRY"):
        """Print a comprehensive summary of all metrics."""
        print(f"\n{'â•' * 70}")
        print(f"  {title}")
        print(f"{'â•' * 70}")

        print("\n  ðŸ“Š MEMORY ORACLE:")
        print(f"    Total accesses:      {self.oracle_total_accesses:>12,}")
        print(f"    Predictions made:    {self.oracle_predictions:>12,}")
        print(f"    Prefetch hits:       {self.oracle_hits:>12,}")
        print(f"    Hit rate:            {self.oracle_hit_rate:>11.1%}")
        print(f"    LSTM predictions:    {self.oracle_lstm_predictions:>12,}")
        print(f"    Stride detections:   {self.oracle_stride_detections:>12,}")

        print("\n  ðŸŽ¯ SEMANTIC DISPATCHER:")
        print(f"    Patterns detected:   {self.dispatcher_patterns_detected:>12,}")
        print(f"    Instructions saved:  {self.dispatcher_instructions_saved:>12,}")
        print(f"    Bytes accelerated:   {self.dispatcher_bytes_accelerated:>12,}")
        print(f"    Try dispatch calls:  {self.dispatcher_try_calls:>12,}")
        print(f"    Detection rate:      {self.dispatcher_detection_rate:>11.1%}")

        print("\n  ðŸ›¡ï¸ SAFETY:")
        print(f"    Bounds violations:   {self.safety_bounds_violations:>12,}")
        print(f"    Null page rejects:   {self.safety_null_page_rejections:>12,}")
        print(f"    Overflow rejects:    {self.safety_overflow_rejections:>12,}")
        print(f"    Prefetch rejects:    {self.safety_prefetch_rejections:>12,}")

        print("\n  âš¡ ADAPTIVE THRESHOLD:")
        print(f"    Current threshold:   {self.adaptive_current_threshold:>11.2f}")
        print(f"    Adaptations made:    {self.adaptive_adaptations:>12,}")
        print(f"    Window hit rate:     {self.adaptive_window_hit_rate:>11.1%}")

        print("\n  ðŸ§  TRAINED LSTM:")
        print(f"    Model loaded:        {'âœ… Yes' if self.trained_model_loaded else 'âŒ No':>12}")
        if self.trained_model_loaded:
            print(f"    Pattern accuracy:    {self.trained_model_pattern_accuracy:>11.1%}")
            print(f"    Current pattern:     {self.trained_current_pattern:>12}")
            print(f"    Pattern confidence:  {self.trained_pattern_confidence:>11.1%}")

        print(f"\n{'â•' * 70}")


class MemoryPatternEncoder(nn.Module):
    """
    Encodes memory access patterns into dense feature vectors.

    Converts raw address deltas and access types into learned embeddings
    that capture the semantic meaning of access patterns.
    """

    def __init__(self, feature_dim: int = 64, device=None):
        super().__init__()
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
        self.feature_dim = feature_dim

        # Address delta encoder (captures stride patterns)
        # Input: [delta, log2(abs(delta)+1), sign, access_type]
        self.delta_encoder = nn.Sequential(
            nn.Linear(8, 32),
            nn.ReLU(),
            nn.Linear(32, feature_dim),
        ).to(self.device)

        # Access type embedding (load vs store, size: 1/2/4/8 bytes)
        self.access_embed = nn.Embedding(16, 16).to(self.device)  # 4 sizes Ã— 2 types Ã— 2 (signed/unsigned)

        # Combine delta and access type
        self.combiner = nn.Linear(feature_dim + 16, feature_dim).to(self.device)

    def encode_access(self, addr: int, prev_addr: int, access_type: int = 0) -> torch.Tensor:
        """
        Encode a single memory access into feature vector.

        Args:
            addr: Current access address
            prev_addr: Previous access address
            access_type: 0-15 encoding access characteristics

        Returns:
            Feature vector [feature_dim]
        """
        delta = addr - prev_addr
        abs_delta = abs(delta)

        # Create feature vector on GPU
        features = torch.tensor([
            delta / 1e6,  # Normalized delta
            torch.log2(torch.tensor(abs_delta + 1.0)).item(),  # Log scale
            1.0 if delta >= 0 else -1.0,  # Sign
            1.0 if abs_delta <= 8 else 0.0,  # Sequential indicator
            1.0 if abs_delta in [8, 16, 24, 32] else 0.0,  # Common struct stride
            1.0 if abs_delta % 4 == 0 else 0.0,  # Word aligned
            1.0 if abs_delta % 8 == 0 else 0.0,  # Double-word aligned
            float(access_type & 0x7) / 7.0,  # Access size encoding
        ], dtype=torch.float32, device=self.device)

        # Encode delta
        delta_feat = self.delta_encoder(features)

        # Encode access type
        access_feat = self.access_embed(torch.tensor([access_type], device=self.device))

        # Combine
        combined = torch.cat([delta_feat, access_feat.squeeze(0)])
        return self.combiner(combined)


class MemoryAccessPredictor(nn.Module):
    """
    LSTM-based memory access predictor.

    Learns to predict the next N memory addresses based on the history
    of recent accesses. Key innovation: predicts DELTAS not absolute addresses,
    which generalizes much better across different memory regions.
    """

    def __init__(
        self,
        hidden_size: int = 128,
        num_layers: int = 2,
        history_len: int = 32,
        lookahead: int = 8,
        feature_dim: int = 64,
        device=None
    ):
        super().__init__()
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.history_len = history_len
        self.lookahead = lookahead
        self.feature_dim = feature_dim

        # Pattern encoder
        self.encoder = MemoryPatternEncoder(feature_dim, device=self.device)

        # LSTM for sequence modeling
        self.lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1 if num_layers > 1 else 0,
        ).to(self.device)

        # Predict next N deltas (regression)
        self.delta_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, lookahead),
        ).to(self.device)

        # Predict confidence for each prediction
        self.confidence_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, lookahead),
            nn.Sigmoid(),
        ).to(self.device)

        # Pattern classifier (helps with stride detection)
        # 0=random, 1=sequential, 2=strided, 3=pointer-chase
        self.pattern_classifier = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Linear(32, 4),
            nn.Softmax(dim=-1),
        ).to(self.device)

        # Hidden state for online learning
        self.hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None

        # History buffer (GPU-resident ring buffer)
        self.history_features = torch.zeros(history_len, feature_dim, device=self.device)
        self.history_addrs = torch.zeros(history_len, dtype=torch.int64, device=self.device)
        self.history_idx = 0
        self.history_count = 0

        # Last address for delta computation
        self.last_addr = 0

        # Statistics
        self.stats = PrefetchStats()

        # PERFORMANCE OPTIMIZATION: Lazy feature encoding
        # Store raw addresses in CPU list (fast), encode to GPU only when needed
        self._raw_addr_history = []  # CPU list for fast appends
        self._raw_access_types = []  # CPU list
        self._features_dirty = True  # True = features need recomputation

    def reset_state(self):
        """Reset LSTM hidden state (e.g., at context switch)."""
        self.hidden = None
        self.history_idx = 0
        self.history_count = 0
        self.last_addr = 0
        # Clear raw history for lazy encoding
        self._raw_addr_history = []
        self._raw_access_types = []
        self._features_dirty = True

    def record_access(self, addr: int, access_type: int = 0):
        """
        Record a memory access to the history buffer.

        PERFORMANCE OPTIMIZED: This is called for every memory operation and must be FAST.
        We only store raw addresses here - ALL GPU operations happen lazily in batches.
        """
        self.stats.total_accesses += 1

        # FAST PATH: Just store raw address in CPU list (nanoseconds, not milliseconds!)
        self._raw_addr_history.append(addr)
        self._raw_access_types.append(access_type)

        # Keep history bounded
        if len(self._raw_addr_history) > self.history_len:
            self._raw_addr_history.pop(0)
            self._raw_access_types.pop(0)

        # Track count for prediction threshold (no GPU ops!)
        self.history_count = min(len(self._raw_addr_history), self.history_len)

        # Mark features as dirty - will be recomputed when needed
        self._features_dirty = True
        self.last_addr = addr

    @torch.no_grad()
    def _encode_features_batch(self):
        """
        Batch-encode all stored addresses into features.

        Called lazily when predictions are needed - much faster than per-access encoding.
        """
        if not self._features_dirty or len(self._raw_addr_history) < 2:
            return

        # Batch encode all addresses at once
        prev_addr = 0
        for i, (addr, access_type) in enumerate(zip(self._raw_addr_history, self._raw_access_types)):
            if i < self.history_len:
                features = self.encoder.encode_access(addr, prev_addr, access_type)
                self.history_features[i] = features
                self.history_addrs[i] = addr
            prev_addr = addr

        # Update history index to match the batch
        self.history_idx = len(self._raw_addr_history) % self.history_len
        self._features_dirty = False

    @torch.no_grad()
    def predict_next(self, current_addr: int) -> Tuple[List[int], List[float]]:
        """
        Predict the next N memory addresses.

        Returns:
            addresses: List of predicted absolute addresses
            confidences: List of confidence scores [0-1]
        """
        if self.history_count < 4:
            # Not enough history yet
            return [], []

        # Lazily encode features if needed
        self._encode_features_batch()

        # Get ordered history (handle ring buffer wrap)
        if self.history_count < self.history_len:
            features = self.history_features[:self.history_count].unsqueeze(0)
        else:
            # Reorder to chronological
            idx = self.history_idx
            features = torch.cat([
                self.history_features[idx:],
                self.history_features[:idx]
            ]).unsqueeze(0)

        # Run LSTM
        output, self.hidden = self.lstm(features, self.hidden)
        final_hidden = output[0, -1, :]  # Last timestep

        # Predict deltas and confidences
        deltas = self.delta_predictor(final_hidden)
        confidences = self.confidence_predictor(final_hidden)

        # Convert deltas to absolute addresses
        # Scale deltas back (they were normalized during encoding)
        predicted_deltas = deltas * 1e6
        addresses = [current_addr + int(d.item()) for d in predicted_deltas]
        conf_list = [c.item() for c in confidences]

        self.stats.predictions_made += len(addresses)

        return addresses, conf_list

    @torch.no_grad()
    def detect_pattern(self) -> Tuple[str, float]:
        """
        Detect the current memory access pattern.

        Returns:
            pattern_name: 'random', 'sequential', 'strided', 'pointer-chase'
            confidence: Pattern detection confidence
        """
        if self.history_count < 4:
            return 'unknown', 0.0

        # Lazily encode features if needed
        self._encode_features_batch()

        # Get history features
        if self.history_count < self.history_len:
            features = self.history_features[:self.history_count].unsqueeze(0)
        else:
            idx = self.history_idx
            features = torch.cat([
                self.history_features[idx:],
                self.history_features[:idx]
            ]).unsqueeze(0)

        # Run LSTM
        output, _ = self.lstm(features)
        final_hidden = output[0, -1, :]

        # Classify pattern
        probs = self.pattern_classifier(final_hidden)
        pattern_idx = torch.argmax(probs).item()
        confidence = probs[pattern_idx].item()

        patterns = ['random', 'sequential', 'strided', 'pointer-chase']
        return patterns[pattern_idx], confidence


class TrainedMemoryOracleLSTM(nn.Module):
    """
    LSTM model matching the EXACT architecture from train_memory_oracle.py.

    This class must precisely match the trained checkpoint's architecture:
    - input_dim=10 feature vector per access
    - 64-dim feature encoder
    - 2-layer LSTM with 128 hidden size
    - Separate heads for delta prediction, confidence, and pattern classification
    """

    def __init__(
        self,
        input_dim: int = 10,
        hidden_size: int = 128,
        num_layers: int = 2,
        lookahead: int = 8,
        num_patterns: int = 4,
        dropout: float = 0.1,
        device=None
    ):
        super().__init__()
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lookahead = lookahead
        self.input_dim = input_dim

        # Feature encoder (must match train_memory_oracle.py exactly)
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
        )

        # LSTM
        self.lstm = nn.LSTM(
            input_size=64,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )

        # Delta predictor head
        self.delta_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, lookahead),
        )

        # Confidence predictor head
        self.confidence_head = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, lookahead),
            nn.Sigmoid()
        )

        # Pattern classifier head
        self.pattern_head = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, num_patterns)
        )

        # Hidden state for incremental inference
        self.hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None

    def forward(self, x: torch.Tensor, return_hidden: bool = False):
        """
        Forward pass.

        Args:
            x: Input features [batch, seq_len, input_dim]
            return_hidden: Whether to return LSTM hidden states

        Returns:
            delta_preds: Predicted deltas [batch, seq_len, lookahead]
            confidence: Confidence scores [batch, seq_len, lookahead]
            pattern_logits: Pattern classification [batch, num_patterns]
            hidden: (optional) LSTM hidden state
        """
        # Encode features
        encoded = self.feature_encoder(x)  # [batch, seq_len, 64]

        # LSTM
        lstm_out, hidden = self.lstm(encoded, self.hidden if not self.training else None)

        # Predict deltas from each position
        delta_preds = self.delta_head(lstm_out)  # [batch, seq_len, lookahead]
        confidence = self.confidence_head(lstm_out)  # [batch, seq_len, lookahead]

        # Pattern classification from final hidden state
        final_hidden = lstm_out[:, -1, :]  # [batch, hidden]
        pattern_logits = self.pattern_head(final_hidden)  # [batch, num_patterns]

        if return_hidden:
            return delta_preds, confidence, pattern_logits, hidden
        return delta_preds, confidence, pattern_logits

    def reset_hidden(self):
        """Reset LSTM hidden state."""
        self.hidden = None

    @torch.no_grad()
    def predict_incremental(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Incremental prediction maintaining hidden state between calls.

        Args:
            features: Single access features [1, 1, input_dim]

        Returns:
            delta_pred: Predicted deltas [lookahead]
            confidence: Confidence scores [lookahead]
            pattern_probs: Pattern probabilities [num_patterns]
        """
        # Encode
        encoded = self.feature_encoder(features)  # [1, 1, 64]

        # LSTM with persistent hidden state
        lstm_out, self.hidden = self.lstm(encoded, self.hidden)

        # Predictions
        delta_pred = self.delta_head(lstm_out[:, -1, :]).squeeze(0)  # [lookahead]
        confidence = self.confidence_head(lstm_out[:, -1, :]).squeeze(0)  # [lookahead]
        pattern_logits = self.pattern_head(lstm_out[:, -1, :]).squeeze(0)  # [num_patterns]
        pattern_probs = torch.softmax(pattern_logits, dim=-1)

        return delta_pred, confidence, pattern_probs


class MemoryOracle:
    """
    High-level Memory Oracle interface.

    Combines the predictor with prefetch logic and caching.
    This is the main interface used by the Neural CPU.
    """

    def __init__(
        self,
        memory_tensor: torch.Tensor,
        history_len: int = 64,
        lookahead: int = 16,
        prefetch_threshold: float = 0.7,
        device=None
    ):
        self.device = device or memory_tensor.device
        self.memory = memory_tensor
        self.memory_size = len(memory_tensor)
        self.prefetch_threshold = prefetch_threshold
        self.lookahead = lookahead

        # Create predictor
        self.predictor = MemoryAccessPredictor(
            hidden_size=128,
            num_layers=2,
            history_len=history_len,
            lookahead=lookahead,
            device=self.device
        )

        # Prefetch buffer (keeps recently predicted addresses warm)
        self.prefetch_cache = set()
        self.max_cache_size = 1024

        # Stride detector for simple patterns (no ML needed)
        self.recent_deltas = []
        self.max_recent = 8

        # Statistics
        self.enabled = True

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ADAPTIVE THRESHOLD CONFIGURATION (Phase 3 addition)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.adaptive_threshold_enabled = True
        self.threshold_min = 0.5               # Minimum prefetch threshold
        self.threshold_max = 0.95              # Maximum prefetch threshold
        self.threshold_adjust_amount = 0.02    # Amount to adjust per adaptation
        self.threshold_adapt_interval = 100    # Adapt every N prefetches

        # Hit/miss tracking for adaptation
        self._hit_miss_window: List[bool] = []  # Rolling window of hit/miss
        self._hit_miss_window_size = 100        # Window size for adaptation
        self._prefetches_since_adapt = 0

        # Adaptive thresholds for triggering adjustment
        self._low_hit_rate_threshold = 0.56    # Below this: increase threshold
        self._high_hit_rate_threshold = 0.84   # Above this: decrease threshold

        # Try to load trained model
        self._load_trained_model()

    def _load_trained_model(self):
        """Attempt to load a trained LSTM model if available."""
        import math
        from pathlib import Path

        # Initialize trained model tracking
        self.trained_lstm: Optional[TrainedMemoryOracleLSTM] = None
        self.trained_model_loaded = False
        self.trained_config = {}
        self.trained_metrics = {}

        # Feature history buffer for trained model (stores last N feature vectors)
        self.trained_feature_history: List[torch.Tensor] = []
        self.trained_history_size = 32  # Same as training window

        # Look for trained model in same directory as this file
        model_path = Path(__file__).parent / "memory_oracle_lstm.pt"
        if not model_path.exists():
            # Also check kvrm-cpu directory
            model_path = Path(__file__).parent.parent / "memory_oracle_lstm.pt"

        if model_path.exists():
            try:
                checkpoint = torch.load(str(model_path), map_location=self.device, weights_only=False)

                # Get config from checkpoint
                config = checkpoint.get('config', {})
                self.trained_config = config

                # Create model with matching architecture
                self.trained_lstm = TrainedMemoryOracleLSTM(
                    input_dim=config.get('input_dim', 10),
                    hidden_size=config.get('hidden_size', 128),
                    num_layers=config.get('num_layers', 2),
                    lookahead=config.get('lookahead', 8),
                    num_patterns=config.get('num_patterns', 4),
                    dropout=0.0,  # No dropout at inference
                    device=self.device
                ).to(self.device)

                # Load trained weights
                self.trained_lstm.load_state_dict(checkpoint['model_state_dict'])
                self.trained_lstm.eval()  # Set to evaluation mode

                self.trained_model_loaded = True
                self.trained_metrics = checkpoint.get('metrics', {})

                # Print info
                pattern_acc = self.trained_metrics.get('pattern_acc', 0)
                print(f"   Memory Oracle: Trained LSTM ACTIVE (pattern acc: {pattern_acc:.1%})")

            except Exception as e:
                self.trained_model_loaded = False
                self.trained_lstm = None
                print(f"   Memory Oracle: Could not load trained model: {e}")
        else:
            print("   Memory Oracle: No trained model found (using stride detection)")

    def _encode_access_for_trained_model(self, addr: int, size: int, is_load: bool, pc: int = 0) -> torch.Tensor:
        """
        Encode a memory access into the 10-feature vector expected by trained LSTM.

        This must match the encoding from train_memory_oracle.py:_encode_sequence().
        """
        import math

        prev_addr = self.predictor.last_addr if self.predictor.last_addr != 0 else addr
        delta = addr - prev_addr
        abs_delta = abs(delta)

        features = torch.tensor([
            delta / 1e6,  # Normalized delta
            math.log1p(abs_delta) / 20.0,  # Log-scale magnitude
            1.0 if delta >= 0 else -1.0,  # Sign
            1.0 if abs_delta <= 8 else 0.0,  # Sequential indicator
            1.0 if abs_delta in [8, 16, 24, 32, 48, 64] else 0.0,  # Common stride
            1.0 if abs_delta % 4 == 0 else 0.0,  # Word aligned
            1.0 if abs_delta % 8 == 0 else 0.0,  # Double-word aligned
            float(min(size, 8)) / 8.0,  # Size encoding
            1.0 if is_load else 0.0,  # Load vs store
            (pc & 0xFF) / 255.0,  # PC low bits
        ], dtype=torch.float32, device=self.device)

        return features

    def _update_trained_feature_history(self, features: torch.Tensor):
        """Add features to the trained model's history buffer."""
        self.trained_feature_history.append(features)
        if len(self.trained_feature_history) > self.trained_history_size:
            self.trained_feature_history.pop(0)

    @torch.no_grad()
    def _predict_with_trained_lstm(self, current_addr: int) -> Tuple[List[int], List[float]]:
        """
        Use the trained LSTM to predict upcoming memory addresses.

        Returns:
            addresses: List of predicted absolute addresses
            confidences: List of confidence scores [0-1]
        """
        import math

        if self.trained_lstm is None or len(self.trained_feature_history) < 4:
            return [], []

        # Stack feature history into batch
        features = torch.stack(self.trained_feature_history).unsqueeze(0)  # [1, seq_len, 10]

        # Run trained model
        delta_preds, confidences, pattern_logits = self.trained_lstm(features)

        # Get predictions from the last position
        delta_pred = delta_preds[0, -1, :]  # [lookahead]
        conf = confidences[0, -1, :]  # [lookahead]

        # Denormalize deltas (reverse the log1p normalization from training)
        # Training uses: delta_signs * log1p(abs(delta)) / 20.0
        # We need to reverse: sign * (exp(abs(pred) * 20) - 1)
        delta_signs = torch.sign(delta_pred)
        delta_mags = torch.expm1(torch.abs(delta_pred) * 20.0)
        denorm_deltas = delta_signs * delta_mags

        # Convert to absolute addresses
        addresses = []
        cumulative_delta = 0
        for d in denorm_deltas:
            cumulative_delta += int(d.item())
            addresses.append(current_addr + cumulative_delta)

        conf_list = [c.item() for c in conf]

        # Track pattern for telemetry
        pattern_probs = torch.softmax(pattern_logits[0], dim=-1)
        pattern_idx = torch.argmax(pattern_probs).item()
        self._last_trained_pattern = ['sequential', 'strided', 'pointer-chase', 'random'][pattern_idx]
        self._last_trained_pattern_conf = pattern_probs[pattern_idx].item()

        return addresses, conf_list

    def get_trained_pattern(self) -> Tuple[str, float]:
        """Get the pattern detected by the trained LSTM."""
        if hasattr(self, '_last_trained_pattern'):
            return self._last_trained_pattern, self._last_trained_pattern_conf
        return 'unknown', 0.0

    def _record_hit_miss(self, was_hit: bool):
        """Record a hit or miss for adaptive threshold adjustment."""
        self._hit_miss_window.append(was_hit)
        if len(self._hit_miss_window) > self._hit_miss_window_size:
            self._hit_miss_window.pop(0)

        self._prefetches_since_adapt += 1

        # Check if it's time to adapt
        if self._prefetches_since_adapt >= self.threshold_adapt_interval:
            self._adapt_threshold()
            self._prefetches_since_adapt = 0

    def _adapt_threshold(self):
        """
        Adapt the prefetch threshold based on recent hit/miss history.

        If hit rate is low (<56%): increase threshold to be more conservative
        If hit rate is high (>84%): decrease threshold to prefetch more aggressively
        """
        if not self.adaptive_threshold_enabled:
            return

        if len(self._hit_miss_window) < 20:
            return  # Not enough data to adapt

        # Calculate current hit rate
        hits = sum(1 for x in self._hit_miss_window if x)
        hit_rate = hits / len(self._hit_miss_window)

        old_threshold = self.prefetch_threshold

        # Adapt threshold based on hit rate
        if hit_rate < self._low_hit_rate_threshold:
            # Too many misses - increase threshold (be more conservative)
            self.prefetch_threshold = min(
                self.threshold_max,
                self.prefetch_threshold + self.threshold_adjust_amount
            )
        elif hit_rate > self._high_hit_rate_threshold:
            # Good hit rate - decrease threshold (be more aggressive)
            self.prefetch_threshold = max(
                self.threshold_min,
                self.prefetch_threshold - self.threshold_adjust_amount
            )

        # Track adaptation
        if not hasattr(self, '_threshold_adaptations'):
            self._threshold_adaptations = []
        if old_threshold != self.prefetch_threshold:
            self._threshold_adaptations.append({
                'from': old_threshold,
                'to': self.prefetch_threshold,
                'hit_rate': hit_rate,
                'window_size': len(self._hit_miss_window),
            })

    def get_adaptive_threshold_stats(self) -> dict:
        """Get statistics about adaptive threshold behavior."""
        if len(self._hit_miss_window) > 0:
            current_hit_rate = sum(1 for x in self._hit_miss_window if x) / len(self._hit_miss_window)
        else:
            current_hit_rate = 0.0

        return {
            'current_threshold': self.prefetch_threshold,
            'adaptive_enabled': self.adaptive_threshold_enabled,
            'current_hit_rate': current_hit_rate,
            'window_size': len(self._hit_miss_window),
            'adaptations_count': len(getattr(self, '_threshold_adaptations', [])),
            'recent_adaptations': getattr(self, '_threshold_adaptations', [])[-5:],
        }

    def record_load(self, addr: int, size: int = 8, pc: int = 0):
        """Record a load operation."""
        if not self.enabled:
            return

        # Track for stride detection (fast path - simple Python)
        if self.predictor.last_addr != 0:
            delta = addr - self.predictor.last_addr
            self.recent_deltas.append(delta)
            if len(self.recent_deltas) > self.max_recent:
                self.recent_deltas.pop(0)

        # Check if this was prefetched (track hit/miss for adaptive threshold)
        was_prefetched = addr in self.prefetch_cache
        if was_prefetched:
            self.predictor.stats.hits += 1
            self.prefetch_cache.remove(addr)
            self._record_hit_miss(True)  # Record hit for adaptive threshold
        elif len(self.prefetch_cache) > 0:
            # We had prefetches but this wasn't one of them - potential miss
            self._record_hit_miss(False)

        # Record to predictor
        access_type = min(size - 1, 7)  # 0-7 for size 1-8
        self.predictor.record_access(addr, access_type)

        # PERFORMANCE OPTIMIZATION: Only feed LSTM every N accesses
        # Creating tensors on every access is too slow (~2.8ms overhead)
        self._lstm_sample_counter = getattr(self, '_lstm_sample_counter', 0) + 1
        if self.trained_model_loaded and self._lstm_sample_counter >= 50:  # Sample every 50 accesses
            self._lstm_sample_counter = 0
            features = self._encode_access_for_trained_model(addr, size, is_load=True, pc=pc)
            self._update_trained_feature_history(features)

    def record_store(self, addr: int, size: int = 8, pc: int = 0):
        """Record a store operation."""
        if not self.enabled:
            return

        # Stores invalidate prefetch cache for that address
        if addr in self.prefetch_cache:
            self.prefetch_cache.remove(addr)

        # Record to predictor
        access_type = min(size - 1, 7) + 8  # 8-15 for stores
        self.predictor.record_access(addr, access_type)

        # PERFORMANCE OPTIMIZATION: Only feed LSTM every N accesses
        self._lstm_sample_counter = getattr(self, '_lstm_sample_counter', 0) + 1
        if self.trained_model_loaded and self._lstm_sample_counter >= 50:  # Sample every 50 accesses
            self._lstm_sample_counter = 0
            features = self._encode_access_for_trained_model(addr, size, is_load=False, pc=pc)
            self._update_trained_feature_history(features)

    def predict_and_prefetch(self, current_addr: int) -> List[int]:
        """
        Predict upcoming addresses and prefetch them.

        This is the main optimization entry point. Call this periodically
        (e.g., every N instructions) to keep memory warm.

        Prediction priority:
        1. Trained LSTM (94.9% pattern accuracy) - if loaded and confident
        2. Simple stride detection (fast, ~100% on simple patterns)
        3. Untrained LSTM fallback

        Returns:
            List of addresses that were prefetched
        """
        if not self.enabled:
            return []

        # Priority 1: Use trained LSTM if available and has enough history
        if self.trained_model_loaded and len(self.trained_feature_history) >= 4:
            addresses, confidences = self._predict_with_trained_lstm(current_addr)
            if addresses and max(confidences) >= self.prefetch_threshold:
                return self._do_prefetch(addresses, confidences)

        # Priority 2: Simple stride detection (much faster, perfect for regular patterns)
        stride = self._detect_stride()
        if stride is not None:
            addresses = [current_addr + stride * i for i in range(1, self.lookahead + 1)]
            return self._do_prefetch(addresses, [0.95] * len(addresses))

        # Priority 3: Fall back to untrained LSTM prediction
        addresses, confidences = self.predictor.predict_next(current_addr)
        return self._do_prefetch(addresses, confidences)

    def _detect_stride(self) -> Optional[int]:
        """
        Fast stride detection using recent deltas.

        Returns stride if a consistent pattern is detected, None otherwise.
        """
        if len(self.recent_deltas) < 4:
            return None

        # Check if recent deltas are consistent
        recent = self.recent_deltas[-4:]
        if len(set(recent)) == 1 and recent[0] != 0:
            # Perfect stride detected
            return recent[0]

        # Check for alternating pattern (common in struct access)
        if len(self.recent_deltas) >= 6:
            last6 = self.recent_deltas[-6:]
            if last6[0] == last6[2] == last6[4] and last6[1] == last6[3] == last6[5]:
                # Alternating pattern - return sum of pair
                return last6[0] + last6[1]

        return None

    def _validate_prefetch_address(self, addr: int, size: int = 8) -> Tuple[bool, str]:
        """
        Validate a predicted address before prefetching.

        Performs comprehensive bounds checking to prevent:
        - Null pointer access (addresses in null page)
        - Buffer overflow (addresses beyond memory)
        - Negative addresses (underflow)
        - Misaligned accesses (optional warning)

        Args:
            addr: Address to validate
            size: Size of access (default 8 bytes)

        Returns:
            (is_valid, rejection_reason)
        """
        # Track validation statistics
        if not hasattr(self, '_prefetch_rejections'):
            self._prefetch_rejections = {
                'null_page': 0,
                'overflow': 0,
                'underflow': 0,
                'total_rejected': 0,
                'total_validated': 0,
            }

        self._prefetch_rejections['total_validated'] += 1

        # Check 1: Null page protection (avoid 0x0-0x1000 range)
        if addr < 0x1000:
            self._prefetch_rejections['null_page'] += 1
            self._prefetch_rejections['total_rejected'] += 1
            return False, 'null_page'

        # Check 2: Buffer overflow protection
        if addr + size > self.memory_size:
            self._prefetch_rejections['overflow'] += 1
            self._prefetch_rejections['total_rejected'] += 1
            return False, 'overflow'

        # Check 3: Underflow protection (should be caught by null page, but explicit)
        if addr < 0:
            self._prefetch_rejections['underflow'] += 1
            self._prefetch_rejections['total_rejected'] += 1
            return False, 'underflow'

        return True, ''

    def get_prefetch_rejection_stats(self) -> dict:
        """Get statistics about rejected prefetch attempts."""
        if not hasattr(self, '_prefetch_rejections'):
            return {'null_page': 0, 'overflow': 0, 'underflow': 0, 'total_rejected': 0, 'total_validated': 0}
        return self._prefetch_rejections.copy()

    def _do_prefetch(self, addresses: List[int], confidences: List[float]) -> List[int]:
        """
        Actually prefetch the predicted addresses.

        "Prefetching" on GPU means accessing the memory to bring it into
        cache/registers. We do this with a vectorized read.

        Includes comprehensive bounds validation (Phase 3 safety).
        """
        prefetched = []

        for addr, conf in zip(addresses, confidences):
            if conf < self.prefetch_threshold:
                continue

            # Validate address with enhanced bounds checking
            is_valid, reason = self._validate_prefetch_address(addr, size=8)
            if not is_valid:
                continue  # Skip invalid addresses silently

            # Add to cache if space available
            if len(self.prefetch_cache) < self.max_cache_size:
                self.prefetch_cache.add(addr)
                prefetched.append(addr)

        if prefetched:
            # Do a vectorized read to warm the cache
            # This brings the memory into GPU cache even if we don't use it yet
            addrs_tensor = torch.tensor(prefetched, dtype=torch.int64, device=self.device)

            # Additional safety: double-check bounds on tensor
            valid_mask = (addrs_tensor >= 0x1000) & (addrs_tensor < self.memory_size - 7)
            valid_addrs = addrs_tensor[valid_mask]

            if len(valid_addrs) > 0:
                # Touch the memory (this warms the cache)
                _ = self.memory[valid_addrs]
                self.predictor.stats.prefetches_issued += len(valid_addrs)
                self.predictor.stats.bytes_prefetched += len(valid_addrs) * 8

        return prefetched

    def bulk_prefetch_region(self, start_addr: int, size: int):
        """
        Prefetch an entire memory region.

        Useful when we detect we're about to scan a buffer.
        """
        if not self.enabled:
            return

        end_addr = min(start_addr + size, self.memory_size)
        if start_addr >= end_addr:
            return

        # Vectorized read to warm cache
        _ = self.memory[start_addr:end_addr]
        self.predictor.stats.prefetches_issued += 1
        self.predictor.stats.bytes_prefetched += end_addr - start_addr

    def get_stats(self) -> PrefetchStats:
        """Get prefetch statistics."""
        return self.predictor.stats

    def reset_stats(self):
        """Reset statistics."""
        self.predictor.stats = PrefetchStats()

    def reset_state(self):
        """Reset all internal state (e.g., for context switch or new program)."""
        # Reset predictor
        self.predictor.reset_state()

        # Reset stride detection
        self.recent_deltas.clear()

        # Reset prefetch cache
        self.prefetch_cache.clear()

        # Reset trained LSTM state
        if self.trained_model_loaded and self.trained_lstm is not None:
            self.trained_lstm.reset_hidden()
            self.trained_feature_history.clear()

        # Reset pattern tracking
        if hasattr(self, '_last_trained_pattern'):
            del self._last_trained_pattern
            del self._last_trained_pattern_conf

    def get_pattern(self) -> Tuple[str, float]:
        """Get detected memory access pattern (prefers trained LSTM if available)."""
        if self.trained_model_loaded:
            pattern, conf = self.get_trained_pattern()
            if conf > 0.5:  # If trained model is confident
                return pattern, conf

        # Fall back to untrained predictor
        return self.predictor.detect_pattern()

    def get_extended_stats(self) -> dict:
        """Get extended statistics including trained model info."""
        base_stats = self.predictor.stats
        stats = {
            'total_accesses': base_stats.total_accesses,
            'predictions_made': base_stats.predictions_made,
            'hits': base_stats.hits,
            'misses': base_stats.misses,
            'hit_rate': base_stats.hit_rate,
            'coverage': base_stats.coverage,
            'prefetches_issued': base_stats.prefetches_issued,
            'bytes_prefetched': base_stats.bytes_prefetched,
            'trained_model_loaded': self.trained_model_loaded,
            'trained_history_size': len(self.trained_feature_history) if hasattr(self, 'trained_feature_history') else 0,
        }

        if self.trained_model_loaded:
            stats['trained_pattern_accuracy'] = self.trained_metrics.get('pattern_acc', 0)
            pattern, conf = self.get_trained_pattern()
            stats['current_pattern'] = pattern
            stats['pattern_confidence'] = conf

        return stats

    def print_stats(self):
        """Print comprehensive statistics."""
        stats = self.get_extended_stats()
        print(f"\n{'â”€'*50}")
        print("  Memory Oracle Statistics")
        print(f"{'â”€'*50}")
        print(f"  Total accesses:    {stats['total_accesses']:,}")
        print(f"  Predictions made:  {stats['predictions_made']:,}")
        print(f"  Prefetch hits:     {stats['hits']:,}")
        print(f"  Hit rate:          {stats['hit_rate']:.1%}")
        print(f"  Prefetches issued: {stats['prefetches_issued']:,}")
        print(f"  Bytes prefetched:  {stats['bytes_prefetched']:,}")
        print(f"  Trained LSTM:      {'âœ… Active' if stats['trained_model_loaded'] else 'âŒ Not loaded'}")
        if stats['trained_model_loaded']:
            print(f"  Pattern accuracy:  {stats.get('trained_pattern_accuracy', 0):.1%}")
            print(f"  Current pattern:   {stats.get('current_pattern', 'unknown')} ({stats.get('pattern_confidence', 0):.1%})")
        print(f"{'â”€'*50}")


class SemanticPatternDetector:
    """
    Detects high-level semantic patterns in memory access.

    Goes beyond simple stride detection to recognize:
    - String scanning (looking for null terminator)
    - Linked list traversal (pointer chasing)
    - Array reduction (sum, max, min)
    - Hash table lookup

    When a pattern is recognized, we can use specialized GPU kernels
    instead of instruction-by-instruction emulation.
    """

    def __init__(self, memory: torch.Tensor, device=None):
        self.device = device or memory.device
        self.memory = memory

        # Pattern signatures
        self.patterns = {
            'string_scan': self._check_string_scan,
            'memcpy': self._check_memcpy,
            'memset': self._check_memset,
            'linked_list': self._check_linked_list,
            'array_sum': self._check_array_reduction,
        }

        # Access history for pattern matching
        self.access_log = []
        self.max_log = 100

    def record_access(self, addr: int, value: int, is_load: bool, size: int):
        """Record an access for pattern detection."""
        self.access_log.append({
            'addr': addr,
            'value': value,
            'is_load': is_load,
            'size': size
        })
        if len(self.access_log) > self.max_log:
            self.access_log.pop(0)

    def detect_pattern(self) -> Optional[Tuple[str, dict]]:
        """
        Analyze recent accesses and detect patterns.

        Returns:
            (pattern_name, pattern_params) if detected, None otherwise
        """
        if len(self.access_log) < 10:
            return None

        for name, checker in self.patterns.items():
            result = checker()
            if result is not None:
                return (name, result)

        return None

    def _check_string_scan(self) -> Optional[dict]:
        """Check for string scanning pattern (sequential loads checking for 0)."""
        loads = [a for a in self.access_log[-20:] if a['is_load'] and a['size'] == 1]
        if len(loads) < 5:
            return None

        # Check for sequential byte loads
        addrs = [l['addr'] for l in loads]
        deltas = [addrs[i+1] - addrs[i] for i in range(len(addrs)-1)]

        if all(d == 1 for d in deltas):
            return {
                'start': addrs[0],
                'stride': 1,
                'current': addrs[-1]
            }
        return None

    def _check_memcpy(self) -> Optional[dict]:
        """Check for memcpy pattern (alternating load-store at incrementing addresses)."""
        recent = self.access_log[-20:]
        if len(recent) < 10:
            return None

        # Look for load-store pairs
        pairs = []
        for i in range(0, len(recent)-1, 2):
            if recent[i]['is_load'] and not recent[i+1]['is_load']:
                if recent[i]['size'] == recent[i+1]['size']:
                    pairs.append((recent[i]['addr'], recent[i+1]['addr']))

        if len(pairs) < 3:
            return None

        # Check for consistent strides
        src_deltas = [pairs[i+1][0] - pairs[i][0] for i in range(len(pairs)-1)]
        dst_deltas = [pairs[i+1][1] - pairs[i][1] for i in range(len(pairs)-1)]

        if len(set(src_deltas)) == 1 and len(set(dst_deltas)) == 1:
            if src_deltas[0] == dst_deltas[0]:  # Same stride
                return {
                    'src': pairs[0][0],
                    'dst': pairs[0][1],
                    'stride': src_deltas[0],
                    'current_offset': len(pairs) * src_deltas[0]
                }
        return None

    def _check_memset(self) -> Optional[dict]:
        """Check for memset pattern (stores of same value at incrementing addresses)."""
        stores = [a for a in self.access_log[-20:] if not a['is_load']]
        if len(stores) < 5:
            return None

        # Check for same value being stored
        values = [s['value'] for s in stores]
        if len(set(values)) != 1:
            return None

        # Check for sequential addresses
        addrs = [s['addr'] for s in stores]
        deltas = [addrs[i+1] - addrs[i] for i in range(len(addrs)-1)]

        if len(set(deltas)) == 1 and deltas[0] > 0:
            return {
                'start': addrs[0],
                'value': values[0],
                'stride': deltas[0],
                'current': addrs[-1]
            }
        return None

    def _check_linked_list(self) -> Optional[dict]:
        """Check for linked list traversal (load pointer, follow to next)."""
        loads = [a for a in self.access_log[-20:] if a['is_load'] and a['size'] == 8]
        if len(loads) < 3:
            return None

        # In pointer chasing, loaded value becomes next address
        chains = 0
        for i in range(len(loads)-1):
            # Check if loaded value points to next load address (with offset)
            diff = loads[i+1]['addr'] - loads[i]['value']
            if 0 <= diff < 256:  # Reasonable struct offset
                chains += 1

        if chains >= 2:
            return {
                'pattern': 'pointer_chase',
                'last_ptr': loads[-1]['value'],
                'offset': loads[-1]['addr'] - loads[-2]['value'] if len(loads) >= 2 else 0
            }
        return None

    def _check_array_reduction(self) -> Optional[dict]:
        """Check for array reduction (sequential loads, accumulating)."""
        loads = [a for a in self.access_log[-20:] if a['is_load']]
        if len(loads) < 5:
            return None

        # Check for strided access pattern
        addrs = [l['addr'] for l in loads]
        deltas = [addrs[i+1] - addrs[i] for i in range(len(addrs)-1)]

        if len(set(deltas)) == 1 and deltas[0] in [4, 8]:  # Common reduction element sizes
            return {
                'start': addrs[0],
                'stride': deltas[0],
                'current': addrs[-1],
                'element_size': deltas[0]
            }
        return None

    def clear(self):
        """Clear access log."""
        self.access_log.clear()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUICK TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("=" * 60)
    print("  MEMORY ORACLE TEST")
    print("=" * 60)

    # Create test memory
    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Device: {device}")

    memory = torch.zeros(1024 * 1024, dtype=torch.uint8, device=device)
    oracle = MemoryOracle(memory, device=device)

    # Test 1: Sequential access pattern
    print("\n[TEST 1] Sequential access pattern")
    base = 0x1000
    for i in range(100):
        addr = base + i * 8
        oracle.record_load(addr)

    pattern, conf = oracle.get_pattern()
    print(f"  Pattern detected: {pattern} (confidence: {conf:.2%})")

    # Predict next addresses
    predictions, confidences = oracle.predictor.predict_next(base + 100 * 8)
    print(f"  Predictions: {len(predictions)} addresses")
    if predictions:
        expected_next = base + 101 * 8
        error = abs(predictions[0] - expected_next)
        print(f"  First prediction error: {error} bytes")

    # Test 2: Strided access pattern
    print("\n[TEST 2] Strided access (struct field)")
    oracle.predictor.reset_state()
    base = 0x2000
    struct_size = 24
    for i in range(50):
        addr = base + i * struct_size
        oracle.record_load(addr)

    stride = oracle._detect_stride()
    print(f"  Detected stride: {stride} (expected: {struct_size})")

    # Test 3: Prefetch performance
    print("\n[TEST 3] Prefetch simulation")
    oracle.predictor.reset_state()
    oracle.reset_stats()

    base = 0x3000
    for i in range(200):
        addr = base + i * 8

        # Prefetch every 10 iterations
        if i % 10 == 0 and i > 0:
            oracle.predict_and_prefetch(addr)

        oracle.record_load(addr)

    stats = oracle.get_stats()
    print(f"  Total accesses: {stats.total_accesses}")
    print(f"  Predictions made: {stats.predictions_made}")
    print(f"  Hit rate: {stats.hit_rate:.2%}")
    print(f"  Prefetches issued: {stats.prefetches_issued}")
    print(f"  Bytes prefetched: {stats.bytes_prefetched:,}")

    # Test 4: Semantic pattern detection
    print("\n[TEST 4] Semantic pattern detection")
    detector = SemanticPatternDetector(memory, device=device)

    # Simulate string scan
    base = 0x4000
    for i in range(20):
        detector.record_access(base + i, ord('a'), is_load=True, size=1)

    result = detector.detect_pattern()
    if result:
        print(f"  Pattern: {result[0]}")
        print(f"  Params: {result[1]}")
    else:
        print("  No pattern detected")

    print("\n[DONE] Memory Oracle tests complete!")
